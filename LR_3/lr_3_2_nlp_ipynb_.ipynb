{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT3HAVQ4ZMGI"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "In this problem you will develop two techniques for analyzing free text documents: a bag of words approach based on a TFIDF matrix and an n-gram language model.\n",
        "\n",
        "You'll be applying your models to the text from the Federalist Papers.  The Federalist papers were a series of essays written in 1787 and 1788 by Alexander Hamilton, James Madison, and John Jay (they were published anonymously at the time), that promoted the ratification of the U.S. Constitution. If you're curious, you can read more about them here: https://en.wikipedia.org/wiki/The_Federalist_Papers. They are a particularly interesting data set because although the authorship of most of the essays has been long known since around the deaths of Hamilton and Madison, there was still some question about the authorship of certain articles into the 20th century.  You'll use document vectors and language models to do this analysis for yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZhv4BL2ZMGK"
      },
      "outputs": [],
      "source": [
        "import collections # we found collections.Counter and collections.defaultdict useful\n",
        "import itertools\n",
        "import gzip\n",
        "import re\n",
        "import numpy as np\n",
        "import scipy.sparse as sp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py89eeL2ZMGL"
      },
      "source": [
        "## The dataset\n",
        "\n",
        "You'll use a copy of the Federalist Papers downloaded from Project Guttenberg, available here: http://www.gutenberg.org/ebooks/18.  Specifically, the \"pg18.txt.gz\" file included with the homework is the raw text downloaded from Project Guttenberg.  To ensure that everyone starts with the exact same corpus, we are providing you the code to load and tokenize this document, as given below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaH-jCsnZMGM"
      },
      "outputs": [],
      "source": [
        "def load_federalist_corpus(filename=\"pg18.txt.gz\", encoding='utf8'):\n",
        "    \"\"\" Load the federalist papers as a tokenized list of strings\"\"\"\n",
        "    with gzip.open(filename, \"rt\", encoding=encoding) as f:\n",
        "        data = f.read()\n",
        "    papers = data.split(\"FEDERALIST\")\n",
        "\n",
        "    # all start with \"To the people of the State of New York:\" (sometimes . instead of :)\n",
        "    # all end with PUBLIUS (or no end at all)\n",
        "    locations = [(i,[-1] + [m.end()+1 for m in re.finditer(r\"of the State of New York\", p)],\n",
        "                 [-1] + [m.start() for m in re.finditer(r\"PUBLIUS\", p)]) for i,p in enumerate(papers)]\n",
        "    papers_content = [papers[i][max(loc[1]):max(loc[2])] for i,loc in enumerate(locations)]\n",
        "\n",
        "    # discard entries that are not actually a paper\n",
        "    papers_content = [p for p in papers_content if len(p) > 0]\n",
        "\n",
        "    # replace all whitespace with a single space\n",
        "    papers_content = [re.sub(r\"\\s+\", \" \", p).lower() for p in papers_content]\n",
        "\n",
        "    # add spaces before all punctuation, so they are separate tokens\n",
        "    punctuation = set(re.findall(r\"[^\\w\\s]+\", \" \".join(papers_content))) - {\"-\",\"'\"}\n",
        "    for c in sorted(punctuation, reverse=True):\n",
        "        papers_content = [p.replace(c, \" \"+c+\" \") for p in papers_content]\n",
        "    papers_content = [re.sub(r\"\\s+\", \" \", p).lower().strip() for p in papers_content]\n",
        "\n",
        "    authors = [tuple(re.findall(\"MADISON|JAY|HAMILTON\", a)) for a in papers]\n",
        "    authors = [a for a in authors if len(a) > 0]\n",
        "\n",
        "    numbers = [re.search(r\"No\\. \\d+\", p).group(0) for p in papers if re.search(r\"No\\. \\d+\", p)]\n",
        "\n",
        "    return papers_content, authors, numbers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMABRdxUZMGN"
      },
      "source": [
        "You're welcome to dig through the code here if you're curious, but it's more important that you look at the objects that the function returns.  The `papers` object is a list of strings, each one containing the full content of one of the Federalist Papers.  All tokens (words) in the text are separated by a single space (this includes some punctuation tokens, which have been modified to include spaces both before and after the punctuation. The `authors` object is a list of lists, which each list contains the author (or potential authors) of a given paper.  Finally, the `numbers` list just contains the number of each Federalist paper.  You won't need to use this last one, but you may be curious to compare the results of your textual analysis to the opinion of historians."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-t35QTDZMGO"
      },
      "source": [
        "### Q1: Bag of words, and TFIDF\n",
        "\n",
        "In this portion of the question, you'll use a bag of words model to describe the corpus, and write routines to build a TFIDF matrix and a cosine similarity function.  Specifically, you should first implement the TFIDF function below.  **Note that you need to do this manually, and not via the scikit-learn Tfidf vectorizers you used in a previous assignment question**  This should return a _sparse_ TFIDF matrix (as for the Graph Library assignment, make sure to directly create a sparse matrix using `scipy.sparse.coo_matrix()` rather than create a dense matrix and then convert it to be sparse).\n",
        "\n",
        "You should create the tfidf vector using numpy matrix operations and not use an existing implementation.\n",
        "\n",
        "Important: make sure you do _not_ include the empty token `\"\"` as one of your terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQe8TUBLZMGP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "def tfidf(docs):\n",
        "    \"\"\"\n",
        "    Create TFIDF matrix.  This function creates a TFIDF matrix from the\n",
        "    docs input.\n",
        "\n",
        "    Args:\n",
        "        docs: list of strings, where each string represents a space-separated\n",
        "              document\n",
        "\n",
        "    Returns: tuple: (tfidf_matrix, all_words)\n",
        "        tfidf_matrix: sparse matrix (in any scipy sparse format) of size (# docs) x\n",
        "               (# total unique words), where i,j entry is TFIDF score for\n",
        "               document i and term j\n",
        "        all_words: list of strings, where the ith element indicates the word\n",
        "                   that corresponds to the ith column in the TFIDF matrix\n",
        "    \"\"\"\n",
        "    # Create a list of all unique words across all documents\n",
        "    all_words = sorted(set(word for doc in docs for word in doc.split() if word != \"\"))\n",
        "\n",
        "    # Create a mapping from word to index\n",
        "    word_to_index = {word: i for i, word in enumerate(all_words)}\n",
        "\n",
        "    # Calculate the document frequency (DF) for each word\n",
        "    doc_freq = {word: 0 for word in all_words}\n",
        "    for doc in docs:\n",
        "        unique_words_in_doc = set(doc.split())\n",
        "        for word in unique_words_in_doc:\n",
        "            if word != \"\":\n",
        "                doc_freq[word] += 1\n",
        "\n",
        "    # Total number of documents\n",
        "    num_docs = len(docs)\n",
        "\n",
        "    # Calculate the inverse document frequency (IDF)\n",
        "    idf = {}\n",
        "    for word, freq in doc_freq.items():\n",
        "        if freq > 0:\n",
        "            idf[word] = math.log(num_docs / freq)\n",
        "        else:\n",
        "            idf[word] = 0.0  # If word doesn't appear in any document, IDF is 0 (log(0) -> 0)\n",
        "\n",
        "    # Prepare the sparse matrix components (rows, cols, values)\n",
        "    rows = []\n",
        "    cols = []\n",
        "    values = []\n",
        "\n",
        "    # For each document, calculate TF-IDF\n",
        "    for i, doc in enumerate(docs):\n",
        "        word_counts = Counter(doc.split())\n",
        "        doc_len = len(doc.split())\n",
        "\n",
        "        for word, count in word_counts.items():\n",
        "            if word != \"\":  # Exclude empty tokens\n",
        "                tf = count / doc_len\n",
        "                tfidf_value = tf * idf[word]\n",
        "                rows.append(i)\n",
        "                cols.append(word_to_index[word])\n",
        "                values.append(tfidf_value)\n",
        "\n",
        "    # Create the sparse matrix using scipy.sparse.coo_matrix\n",
        "    tfidf_matrix = sp.coo_matrix((values, (rows, cols)), shape=(num_docs, len(all_words)))\n",
        "\n",
        "    return tfidf_matrix, all_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZYstdTmZMGP"
      },
      "source": [
        "Our version results the following result (just showing the type, size, and # of non-zero elements):\n",
        "\n",
        "    <86x8686 sparse matrix of type '<type 'numpy.float64'>'\n",
        "        with 57607 stored elements in Compressed Sparse Row format>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha9_wiuwZMGQ"
      },
      "source": [
        "### Q2: Cosine Similarity\n",
        "\n",
        "Next, implement the following simple function that takes the X_tfidf matrix (though it could also take simple term frequency matrices, etc), and compute a matrix of all pair-wise cosine similarities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPZQ8uKSZMGQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# @mugrade.local_tests\n",
        "def cosine_similarity(X):\n",
        "    \"\"\"\n",
        "    Return a matrix of cosine similarities.\n",
        "\n",
        "    Args:\n",
        "        X: sparse matrix of TFIDF scores or term frequencies\n",
        "\n",
        "    Returns:\n",
        "        M: dense numpy array of all pairwise cosine similarities.  That is, the\n",
        "           entry M[i,j], should correspond to the cosine similarity between the\n",
        "           ith and jth rows of X.\n",
        "    \"\"\"\n",
        "    # Ensure the input is a CSR sparse matrix\n",
        "    X = csr_matrix(X)\n",
        "\n",
        "    # Compute the dot product matrix (X * X.T)\n",
        "    dot_product_matrix = X.dot(X.T).toarray()\n",
        "\n",
        "    # Compute the L2 norm for each document (row)\n",
        "    norms = np.linalg.norm(X.toarray(), axis=1)\n",
        "\n",
        "    # Normalize the dot product matrix by the product of the norms\n",
        "    norms_matrix = np.outer(norms, norms)\n",
        "\n",
        "    # Compute the cosine similarity matrix\n",
        "    similarity_matrix = dot_product_matrix / norms_matrix\n",
        "\n",
        "    # Return the similarity matrix\n",
        "    return similarity_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9qav09_ZMGR"
      },
      "source": [
        "### Q3 Analyzing document authorship\n",
        "\n",
        "Finally, use this model to analyze potential authorship of the unknown Federalist Papers.  Specifically, compute the average cosine similarity between all the _known_ Hamilton papers and all the _unknown_ papers (and similarly between known Madison and unknown, and Jay and unknown).  Fill out the following function to compute and return these averages.\n",
        "\n",
        "Hints:\n",
        "\n",
        "1. fit a single TFIDF encoding to all papers and transform all papers using it before computing the similarity measure\n",
        "2. for the cosine similarity to be useful when comparing documents, they must all be encoded the same way. Transform all papers together before comparing cosine similarity.\n",
        "3. the unknown papers have author=`(\"HAMILTON\",\"MADISON\")`;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PHb6jioZMGR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import vstack\n",
        "\n",
        "def author_cosine_similarity(docs, authors):\n",
        "    \"\"\"\n",
        "    Return a tuple of average cosine similarities between all the known papers for a given author\n",
        "    and all the unknown papers.\n",
        "\n",
        "    Args:\n",
        "        docs: list of strings, where each string represents a space-separated\n",
        "              document\n",
        "        authors: list of lists, which each list contains the author (or potential authors) of a given document\n",
        "\n",
        "    Returns: tuple: (hamilton_mcs, madison_mcs, jay_mcs)\n",
        "        hamilton_mcs: Average cosine similarity between all the known Hamilton papers and all the unknown papers.\n",
        "        madison_mcs: Average cosine similarity between all the known Madison papers and all the unknown papers.\n",
        "        jay_mcs: Average cosine similarity between all the known Jay papers and all the unknown papers.\n",
        "    \"\"\"\n",
        "    # Step 1: Preprocess the documents using TF-IDF\n",
        "    tfidf_matrix, all_words = tfidf(docs)\n",
        "\n",
        "    # Step 2: Split the papers into known and unknown sets based on authorship\n",
        "    hamilton_papers = []\n",
        "    madison_papers = []\n",
        "    jay_papers = []\n",
        "    unknown_papers = []\n",
        "\n",
        "    for i, author_list in enumerate(authors):\n",
        "        if 'HAMILTON' in author_list:\n",
        "            hamilton_papers.append(tfidf_matrix[i])\n",
        "        elif 'MADISON' in author_list:\n",
        "            madison_papers.append(tfidf_matrix[i])\n",
        "        elif 'JAY' in author_list:\n",
        "            jay_papers.append(tfidf_matrix[i])\n",
        "        else:\n",
        "            unknown_papers.append(tfidf_matrix[i])\n",
        "\n",
        "    # Convert lists of sparse matrices to single sparse matrix using vstack\n",
        "    hamilton_papers = vstack(hamilton_papers)\n",
        "    madison_papers = vstack(madison_papers)\n",
        "    jay_papers = vstack(jay_papers)\n",
        "    unknown_papers = vstack(unknown_papers)\n",
        "\n",
        "    # Step 3: Compute cosine similarity for each author\n",
        "    hamilton_mcs = np.mean(cosine_similarity(hamilton_papers, unknown_papers))\n",
        "    madison_mcs = np.mean(cosine_similarity(madison_papers, unknown_papers))\n",
        "    jay_mcs = np.mean(cosine_similarity(jay_papers, unknown_papers))\n",
        "\n",
        "    # Step 4: Return the results\n",
        "    return hamilton_mcs, madison_mcs, jay_mcs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJN4Z_SPZMGR"
      },
      "source": [
        "Run the local test case, `author_cosine_similarity(*load_federalist_corpus()[:2])`, to see which author has the highest degree of similarity with the unknown essays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXXt3mMEZMGS"
      },
      "source": [
        "\n",
        "## N-gram language model\n",
        "\n",
        "### Q4 Building an n-gram language model\n",
        "\n",
        "In this question, you will implement an n-gram model to be able to model the language used in the Federalist Papers in a more structured manner than the simple bag of words approach.  You will fill in the following class, and should do it in two parts:\n",
        "\n",
        "### Part A: Initializing the language model\n",
        "\n",
        "First, implement the `__init__()` function in the `LanguageModel` class.  You should do this by building a two-level dictionary (in fact, we used the `collections.defaultdict` class, but this only make a few things a little bit shorter ... you are free to use it or not) `self.counts`, where the first key refers to the previous $n-1$ tokens, and the second key refers to the $n$-th token, and the value simply stores the count of the number of times this combination was seen.  For ease of use in later functions, we also created a `self.count_sums`, which contained the number of total times each $n-1$ combination was seen. We also build a `self.vocabulary` variable, which is just a `set` object containing all the unique words across the entire set of the input document.\n",
        "\n",
        "### Part B: Computing perplexity\n",
        "\n",
        "Next, implement the `perplexity()` function, which takes a text sample and computes the perplexity of this sample under the model.  We'll make a small tweak to the formula for perplexity in the notes because we are skipping the first $n-1$ tokens; since we only compute (#words - $(n-1)$) terms in the sum, you should divide by this denominator and not just #words). Be careful to not multiply together probabilities that get too small (hint: instead of taking the log of a large product, take the sum of the logs).\n",
        "\n",
        "Specifically,\n",
        "\n",
        "\\begin{equation}\n",
        "\\mbox{Perplexity} = 2^{\\frac{-\\log_2 P\\left(\\mathrm{word}_1, \\ldots, \\mathrm{word}_N\\right)}{N-(n+1)}} = 2^{\\text{^}}\\left(\\frac{-\\log_2 P(\\mathrm{word}_1, \\ldots, \\mathrm{word}_N)}{N-(n-1)}\\right)\n",
        "\\end{equation}\n",
        "\n",
        "where\n",
        "\n",
        "\\begin{equation}\n",
        "\\log_2 P(\\mathrm{word}_1, \\ldots, \\mathrm{word}_N) = \\sum_{i=n}^N \\log_2 P(\\mathrm{word}_i | \\mathrm{word}_{i-n+1}, \\ldots, \\mathrm{word}_{i-1})\n",
        "\\end{equation}\n",
        "\n",
        "You'll want to be careful about vocabulary sizes when it comes to the Laplace smoothing term: make sure your vocabulary size $D$ is equal to the total number of unique words that occur in either the original data used to build the language model _or_ in the text we are evaluating the perplexity of (so the size of the union of the two)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWgDQCjwZMGS"
      },
      "outputs": [],
      "source": [
        "class LanguageModel:\n",
        "    def __init__(self, docs, n):\n",
        "        \"\"\"\n",
        "        Initialize an n-gram language model.\n",
        "\n",
        "        Args:\n",
        "            docs: list of strings, where each string represents a space-separated\n",
        "                  document\n",
        "            n: integer, degree of n-gram model\n",
        "        \"\"\"\n",
        "\n",
        "        self.n = n\n",
        "        self.counts = collections.defaultdict(lambda: collections.defaultdict(int))\n",
        "        self.count_sums = collections.defaultdict(int)\n",
        "        self.vocabulary = set()\n",
        "\n",
        "        # Process each document\n",
        "        for doc in docs:\n",
        "            tokens = doc.split()  # Split the document into words\n",
        "            self.vocabulary.update(tokens)\n",
        "\n",
        "            for i in range(len(tokens) - n + 1):\n",
        "                context = tuple(tokens[i:i + n - 1])  # (n-1) previous tokens\n",
        "                target = tokens[i + n - 1]  # nth token\n",
        "\n",
        "                self.counts[context][target] += 1\n",
        "                self.count_sums[context] += 1  # Increment total count for this context\n",
        "\n",
        "        self.vocabulary = set(self.vocabulary)\n",
        "\n",
        "    def perplexity(self, text, alpha=1e-3):\n",
        "        \"\"\"\n",
        "        Evaluate perplexity of model on some text.\n",
        "\n",
        "        Args:\n",
        "            text: string containing space-separated words, on which to compute\n",
        "            alpha: constant to use in Laplace smoothing\n",
        "\n",
        "        Note: for the purposes of smoothing, the vocabulary size (i.e, the D term)\n",
        "        should be equal to the total number of unique words used to build the model\n",
        "        _and_ in the input text to this function.\n",
        "\n",
        "        Returns: perplexity\n",
        "            perplexity: floating point value, perplexity of the text as evaluated\n",
        "                        under the model.\n",
        "        \"\"\"\n",
        "        # Split the text into words\n",
        "        tokens = text.split()\n",
        "        N = len(tokens)\n",
        "\n",
        "        # Initialize log probability sum\n",
        "        log_prob_sum = 0\n",
        "\n",
        "        # Process each word starting from the (n-1)th index\n",
        "        for i in range(self.n - 1, N):\n",
        "            context = tuple(tokens[i - self.n + 1:i])  # Last (n-1) words as context\n",
        "            target = tokens[i]  # The nth word\n",
        "\n",
        "            # Calculate the probability using Laplace smoothing\n",
        "            context_count = self.count_sums[context]\n",
        "            target_count = self.counts[context].get(target, 0)\n",
        "\n",
        "            # Laplace smoothing: add alpha to all counts\n",
        "            probability = (target_count + alpha) / (context_count + alpha * len(self.vocabulary))\n",
        "\n",
        "            # Add the log probability to the sum (log base 2)\n",
        "            log_prob_sum += math.log2(probability)\n",
        "\n",
        "        # Calculate the perplexity\n",
        "        perp = 2 ** (-log_prob_sum / (N - (self.n - 1)))\n",
        "        return perp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUafY9zJZMGT"
      },
      "source": [
        "### Q5 Author identification via language models\n",
        "\n",
        "Using this model, evaluate the mean of the perplexity of the unknown Federalist papers for the language models from each of the three authors (again, using `n=3` and the default of `alpha=1e-3`). Fill in the following function to calculate and return the mean perplexities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H8TJ9KzZMGT"
      },
      "outputs": [],
      "source": [
        "def mean_perplexity(docs, authors):\n",
        "    \"\"\"\n",
        "    Evaluate the mean of the perplexity of the unknown Federalist papers for the language models\n",
        "    from each of the three authors (again, using n=3 and alpha=1e-3)\n",
        "\n",
        "    Args:\n",
        "        docs: list of strings, where each string represents a space-separated document\n",
        "        authors: list of lists, which each list contains the author (or potential authors) of a given document\n",
        "\n",
        "    Returns: tuple: (perp_hamilton, perp_madison, perp_jay)\n",
        "        perp_hamilton: floating point value, mean perplexity of the unknown Federalist papers for the language\n",
        "                       models from Hamilton\n",
        "        perp_madison: floating point value, mean perplexity of the unknown Federalist papers for the language\n",
        "                       models from Madison\n",
        "        perp_jay: floating point value, mean perplexity of the unknown Federalist papers for the language\n",
        "                       models from Jay\n",
        "    \"\"\"\n",
        "     # Separate known and unknown papers for each author\n",
        "    hamilton_docs = []\n",
        "    madison_docs = []\n",
        "    jay_docs = []\n",
        "    unknown_docs = []\n",
        "\n",
        "    for doc, author in zip(docs, authors):\n",
        "        if 'HAMILTON' in author:\n",
        "            hamilton_docs.append(doc)\n",
        "        if 'MADISON' in author:\n",
        "            madison_docs.append(doc)\n",
        "        if 'JAY' in author:\n",
        "            jay_docs.append(doc)\n",
        "        if 'HAMILTON' not in author and 'MADISON' not in author and 'JAY' not in author:\n",
        "            unknown_docs.append(doc)\n",
        "\n",
        "    # Initialize language models for each author using their known documents\n",
        "    lm_hamilton = LanguageModel(hamilton_docs, n=3)\n",
        "    lm_madison = LanguageModel(madison_docs, n=3)\n",
        "    lm_jay = LanguageModel(jay_docs, n=3)\n",
        "\n",
        "    # Calculate the mean perplexity for each author\n",
        "    perp_hamilton = sum(lm_hamilton.perplexity(doc) for doc in unknown_docs) / len(unknown_docs)\n",
        "    perp_madison = sum(lm_madison.perplexity(doc) for doc in unknown_docs) / len(unknown_docs)\n",
        "    perp_jay = sum(lm_jay.perplexity(doc) for doc in unknown_docs) / len(unknown_docs)\n",
        "\n",
        "    return perp_hamilton, perp_madison, perp_jay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7ROaYKnZMGT"
      },
      "source": [
        "Does the most likely author (i.e., the author with the smallest perplexity), match up with the author with the highest cosine similarity above?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}